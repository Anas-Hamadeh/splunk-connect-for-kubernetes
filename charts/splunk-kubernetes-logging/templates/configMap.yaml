apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "splunk-kubernetes-logging.fullname" . }}
  labels:
    app: {{ template "splunk-kubernetes-logging.name" . }}
    chart: {{ template "splunk-kubernetes-logging.chart" . }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  fluent.conf: |-
    @include system.conf
    @include source.filesystem.containers.conf
    @include source.filesystem.kube.conf
    @include source.journal.kube.conf
    @include monit.conf
    @include transform.conf
    @include output.conf

  system.conf: |-
    # system wide configurations
    <system>
      log_level {{ or .Values.logLevel .Values.global.logLevel | default "info" }}
      root_dir /tmp/fluentd
    </system>

  source.filesystem.containers.conf: |-
    # This configuration file for Fluentd / td-agent is used
    # to watch changes to Docker log files. The kubelet creates symlinks that
    # capture the pod name, namespace, container name & Docker container ID
    # to the docker logs for pods in the /var/log/containers directory on the host.
    # If running this fluentd configuration in a Docker container, the /var/log
    # directory should be mounted in the container.
    # reading kubelet logs from journal
    #
    # Reference:
    # https://github.com/kubernetes/community/blob/20d2f6f5498a5668bae2aea9dcaf4875b9c06ccb/contributors/design-proposals/node/kubelet-cri-logging.md
    #
    # Json Log Example:
    # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
    # CRI Log Example (not supported):
    # 2016-02-17T00:04:05.931087621Z stdout [info:2016-02-16T16:04:05.930-08:00] Some log text here
    <source>
      @id raw.containers.log
      @type tail
      tag raw.tail.containers.*
      path /var/log/containers/*.log
      pos_file /var/log/splunk-fluentd-containers.log.pos
      path_key _file_path
      read_from_head true
      <parse>
        @type json
        time_key time
        time_type string
        time_format %Y-%m-%dT%H:%M:%S.%NZ
        localtime false
      </parse>
    </source>

    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.tail.containers.**>
      @id detect_exceptions.containers
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>

  source.filesystem.kube.conf: |-
    # This fluentd conf file contains sources for all kinds of kubenetes system components.

    # logs for salt minion, if the kubernetes cluster is configured with salt.
    # Example:
    # 2015-12-21 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081
    <source>
      @id minion
      @type tail
      tag tail.salt
      path /var/log/salt/minion
      pos_file /var/log/splunk-fluentd-salt.pos
      path_key _file_path
      <parse>
        @type regexp
        expression /^(?<time>[^ ]* [^ ,]*)(?<message>.*)$/
        time_key time
        time_type string
        time_format %Y-%m-%d %H:%M:%S
      </parse>
    </source>

    # Example:
    # Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script
    <source>
      @id startupscript.log
      @type tail
      tag tail.startupscript
      path /var/log/startupscript.log
      pos_file /var/log/splunk-fluentd-startupscript.log.pos
      path_key _file_path
      <parse>
        @type syslog
      </parse>
    </source>

    # Examples:
    # time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
    # time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
    <source>
      @id docker.log
      @type tail
      tag tail.docker
      path /var/log/docker.log
      pos_file /var/log/splunk-fluentd-docker.log.pos
      path_key _file_path
      <parse>
        @type regexp
        expression /^time="(?<time>[^)]*)" (?<message>.*)$/
        time_key time
        time_type string
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    # Example:
    # 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal
    <source>
      @id etcd.log
      @type tail
      tag tail.etcd
      path /var/log/etcd.log
      pos_file /var/log/splunk-fluentd-etcd.log.pos
      path_key _file_path
      <parse>
        # Not parsing this, because it doesn't have anything particularly useful to
        # parse out of it (like severities).
        @type none
      </parse>
    </source>

    # Multi-line parsing is required for all the kube logs because very large log
    # statements, such as those that include entire object bodies, get split into
    # multiple lines by glog.

    # Example:
    # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
    <source>
      @id kubelet.log
      @type tail
      tag tail.kubelet
      path /var/log/kubelet.log
      pos_file /var/log/splunk-fluentd-kubelet.log.pos
      path_key _file_path
{{ include "splunk-kubernetes-logging.tail-glog-multiline" . | indent 6 }}
    </source>

    # Example:
    # I1118 21:26:53.975789       6 proxier.go:1096] Port "nodePort for kube-system/default-http-backend:http" (:31429/tcp) was open before and is still needed
    <source>
      @id kube-proxy.log
      @type tail
      tag tail.kube-proxy
      path /var/log/kube-proxy.log
      pos_file /var/log/splunk-fluentd-kube-proxy.log.pos
      path_key _file_path
{{ include "splunk-kubernetes-logging.tail-glog-multiline" . | indent 6 }}
    </source>

    # Example:
    # I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]
    <source>
      @id kube-apiserver.log
      @type tail
      tag tail.kube-apiserver
      path /var/log/kube-apiserver.log
      pos_file /var/log/splunk-fluentd-kube-apiserver.log.pos
      path_key _file_path
{{ include "splunk-kubernetes-logging.tail-glog-multiline" . | indent 6 }}
    </source>

    # Example:
    # I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui
    <source>
      @id kube-controller-manager.log
      @type tail
      tag tail.kube-controller-manager
      path /var/log/kube-controller-manager.log
      pos_file /var/log/splunk-fluentd-kube-controller-manager.log.pos
      path_key _file_path
{{ include "splunk-kubernetes-logging.tail-glog-multiline" . | indent 6 }}
    </source>

    # Example:
    # W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]
    <source>
      @id kube-scheduler.log
      @type tail
      tag tail.kube-scheduler
      path /var/log/kube-scheduler.log
      pos_file /var/log/splunk-fluentd-kube-scheduler.log.pos
      path_key _file_path
{{ include "splunk-kubernetes-logging.tail-glog-multiline" . | indent 6 }}
    </source>

    # Example:
    # I1104 10:36:20.242766       5 rescheduler.go:73] Running Rescheduler
    <source>
      @id rescheduler.log
      @type tail
      tag tail.rescheduler
      path /var/log/rescheduler.log
      pos_file /var/log/splunk-fluentd-rescheduler.log.pos
      path_key _file_path
{{ include "splunk-kubernetes-logging.tail-glog-multiline" . | indent 6 }}
    </source>

    # Example:
    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
    <source>
      @id glbc.log
      @type tail
      tag tail.glbc
      path /var/log/glbc.log
      pos_file /var/log/splunk-fluentd-glbc.log.pos
      path_key _file_path
{{ include "splunk-kubernetes-logging.tail-glog-multiline" . | indent 6 }}
    </source>

    # Example:
    # I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
    <source>
      @id cluster-autoscaler.log
      @type tail
      tag tail.cluster-autoscaler
      path /var/log/cluster-autoscaler.log
      pos_file /var/log/splunk-fluentd-cluster-autoscaler.log.pos
      path_key _file_path
{{ include "splunk-kubernetes-logging.tail-glog-multiline" . | indent 6 }}
    </source>

  source.journal.kube.conf: |-
    # This fluentd conf file contains configurations for reading logs from systemd journal.
    <source>
      @id journald-docker
      tag journal.docker
      filters [{ "_SYSTEMD_UNIT": "docker.service" }]
{{ include "splunk-kubernetes-logging.common-journald-source-conf" . | indent 6 }}
    </source>

    <source>
      @id journald-kubelet
      tag journal.kubelet
      filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
{{ include "splunk-kubernetes-logging.common-journald-source-conf" . | indent 6 }}
    </source>

    <source>
      @id journald-node-problem-detector
      tag journal.node-problem-detector
      filters [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
{{ include "splunk-kubernetes-logging.common-journald-source-conf" . | indent 6 }}
    </source>

  monit.conf: |-
    <source>
      @id fluentd-monitor-agent
      @type monitor_agent
      tag monitor_agent
    </source>

  transform.conf: |-
    # these filters are for generating the source and sourcetype for each event.
    <filter tail.**>
      @type jq_transformer
      jq '{log: .record.log, source: .record._file_path, sourcetype: (if (.tag | startswith("tail.containers.")) then (.record._file_path | split("_") | .[-1] | split("-") | .[:-1] | join("-")) else (.tag | ltrimstr("tail.")) end)} | .sourcetype = "kube:" + .sourcetype | .'
    </filter>

    <filter journal.**>
      @type jq_transformer
      jq '.record.source = "{{ .Values.journalLogPath | default "/run/log/journal" }}/" + .record.source | .record'
    </filter>

    <filter monitor_agent>
      @type jq_transformer
      jq ".record.source = \"namespace:#{ENV['MY_NAMESPACE']}/pod:#{ENV['MY_POD_NAME']}\" | .record.sourcetype = \"fluentd:monitor_agent\" | .record"
    </filter>


  output.conf: |-
    # ignore fluentd's log
    <match fluent.**>
      @type null
    </match>

    <match **>
      @type splunk_hec
      protocol {{ or .Values.splunk.hec.protocol .Values.global.splunk.hec.protocol | default "https" }}
      hec_host {{ required "splunk.hec.host is required." (or .Values.splunk.hec.host .Values.global.splunk.hec.host) }}
      {{- with $hecPort := or .Values.splunk.hec.port .Values.global.splunk.hec.port }}
      {{ if $hecPort }}hec_port {{ $hecPort }}{{ end }}
      {{- end }}
      hec_token "#{ENV['SPLUNK_HEC_TOKEN']}"
      host "#{ENV['SPLUNK_HEC_HOST']}"
      source_key source
      sourcetype_key sourcetype
      {{- if .Values.splunk.hec.indexName }}
      index {{ .Values.splunk.hec.indexName }}
      {{- end }}
      insecure_ssl {{ or .Values.splunk.hec.insecureSSL .Values.global.splunk.hec.insecureSSL | default false }}
      {{- with $clientCert := or .Values.splunk.hec.clientCert .Values.global.splunk.hec.clientCert }}
      {{ if $clientCert }}client_cert /fluentd/etc/splunk/hec_client_cert{{ end }}
      {{- end }}
      {{- with $clientKey := or .Values.splunk.hec.clientKey .Values.global.splunk.hec.clientKey }}
      {{ if $clientKey }}client_key /fluentd/etc/splunk/hec_client_key{{ end }}
      {{- end }}
      {{- with $caFile := or .Values.splunk.hec.caFile .Values.global.splunk.hec.caFile }}
      {{ if $caFile }}ca_file /fluentd/etc/splunk/hec_ca_file{{ end }}
      {{- end }}
      <buffer>
        @type memory
        {{- $limit := .Values.resources.limit }}
        chunk_limit_size {{ if $limit.memory }}{{ template "splunk-kubernetes-logging.convert-memory" $limit.memory }}{{ else }}{{ "500m" }}{{ end }}
        chunk_limit_records 100000
        flush_interval 5s
        flush_thread_count 1
        overflow_action block
        retry_max_times 3
      </buffer>
      <format monitor_agent>
        @type json
      </format>
      <format>
        # we just want to keep the raw logs, not the structure created by container or journal
        @type single_value
        message_key log
        add_newline false
      </format>
    </match>
